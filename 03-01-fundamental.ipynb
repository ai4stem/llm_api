{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77cd6d2-beb6-4443-bee8-ca90d6921a76",
   "metadata": {},
   "source": [
    "# API를 이용해 ChatGPT API 활용하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad58bf-49fd-4dab-b3b2-fefe939e89d9",
   "metadata": {},
   "source": [
    "현재 Python이 실행 중인 가상환경과 일치하는지 확인하기 위해 실행하는 코드이다.   ㅌㅌㅌ\n",
    "정상적이라면 /Path/가상환경명/bin/python과 같은 형태로 출력된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da8b73a-f321-4acc-aa78-0f34c198e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8382e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f8abb0",
   "metadata": {},
   "source": [
    "## ChatGPT API 사용을 위해 미리 해야 할 일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c371e56a",
   "metadata": {},
   "source": [
    "<h3>1. ChatGPT API 키 발급받기</h3>\n",
    "<h3>2. 가상환경 설치하기(anaconda)</h3>\n",
    "Anaconda는 가상환경을 설치하고 지원하는 프로그램이며, 이를 설치하면 별도의 파이썬을 설치하지 않아도 된다.<br>\n",
    "설치 링크 <a href=\"https://www.anaconda.com/download/\">click</a>\n",
    "<h3>3. 나만의 가상환경 만들기</h3>\n",
    "명령 프롬프트나 터미널을 열어서 아래의 명령어를 입력한다.<br>\n",
    "<b>conda create -n llm python=3.11.5</b><br>\n",
    "<b>conda activate llm</b>\n",
    "<b>pip install -r requirements.txt</b><br>\n",
    "깃허브 레포지토리에서 다운로드 받으면 requirements.txt를 받을 수 있으며, 해당 폴더로 이동해 명령어를 실행해야 한다.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab5b15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "import openai\n",
    "from openai._client import OpenAI #ChatGPT API 이용을 위한 라이브러리\n",
    "from dotenv import load_dotenv #.env에서 파일을 읽어오기 위한 환경변수 처리 라이브러리\n",
    "\n",
    "load_dotenv() # 환경 파일 불러오기\n",
    "\n",
    "client = OpenAI(api_key = os.getenv('OPENAI_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010ad33",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 0.0\n",
    "MAX_TOKENS = 16000\n",
    "MODEL = 'gpt-3.5-turbo-0125'\n",
    "context = [] # 리스트\n",
    "\n",
    "def check_tokens(items):\n",
    "    cnt = 0\n",
    "\n",
    "    if items is None:\n",
    "        return cnt\n",
    "\n",
    "    for item in items:\n",
    "        cnt += len(item['content'])\n",
    "\n",
    "    return cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac1ef78",
   "metadata": {},
   "source": [
    "## Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6312c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conversate():\n",
    "    while(1):\n",
    "        message = input('Chat:')\n",
    "        message = message.strip()\n",
    "    \n",
    "        if message == '':\n",
    "            print('Input your talk.')\n",
    "            continue\n",
    "        elif message == 'exit':\n",
    "            break\n",
    "    \n",
    "        # Examine if the size of check is over the maximum tokens\n",
    "        total_cnt = check_tokens(context) + len(message)\n",
    "\n",
    "        if total_cnt >= MAX_TOKENS:\n",
    "            context.clear()\n",
    "            print('context cleared.')\n",
    "\n",
    "        # Setup up for message to call ChatGPT\n",
    "        if len(context) == 0:\n",
    "            context.append({\"role\": \"system\", \"content\": \"You are a helpful assistant.\"})\n",
    "            context.append({\"role\": \"user\", \"content\": message})\n",
    "        else:\n",
    "            context.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        response = client.chat.completions.create(model=MODEL, messages=context, temperature=TEMPERATURE)\n",
    "        answer = response.choices[0].message.content\n",
    "        print(f\"AI: {answer}\")\n",
    "        #codes = markdown.markdown(answer, extensions=['fenced_code', 'codehilite'])\n",
    "        context.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "        if check_tokens(context) >= MAX_TOKENS:\n",
    "            context.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1054f4-24c8-43b4-8f40-1239761b96b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = '파이썬을 공부하려면 어떤 책을 읽으면 좋을까?'\n",
    "\n",
    "messages = []\n",
    "messages.append({\"role\": \"system\", \"content\": \"너는 훌륭한 비서야.\"})\n",
    "messages.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "response = client.chat.completions.create(model=MODEL, messages=messages, temperature=TEMPERATURE)\n",
    "answer = response.choices[0].message.content\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c57dbf-a925-496b-962e-c761f63d34a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2387aa-1d7e-48ae-9485-49f9db0c30ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5c70bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce03015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "context = []\n",
    "\n",
    "def single_ask(message):\n",
    "    message = message.strip()\n",
    "\n",
    "    if message == '':\n",
    "        print('대화 내용을 입력하세요.')\n",
    "    elif message == 'exit':\n",
    "        return\n",
    "\n",
    "    # 대화 맥락을 고려하여 전체 최대 토큰을 초과하는지 체크하도록 한다.\n",
    "    total_cnt = check_tokens(context) + len(message)\n",
    "\n",
    "    if total_cnt >= MAX_TOKENS:\n",
    "        context.clear()\n",
    "        print('context cleared.')\n",
    "\n",
    "    # ChatGPT 대화를 위한 메시지 형태 설정하기\n",
    "    if len(context) == 0:\n",
    "        context.append({\"role\": \"system\", \"content\": \"You are a helpful assistant.\"})\n",
    "        context.append({\"role\": \"user\", \"content\": message})\n",
    "    else:\n",
    "        context.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "    response = client.chat.completions.create(model=MODEL, messages=context, temperature=TEMPERATURE)\n",
    "    answer = response.choices[0].message.content\n",
    "    display(Markdown(answer))  # 마크다운 형태로 해석하여 출력\n",
    "    \n",
    "    context.append({'role': 'assistant', 'content': answer})\n",
    "\n",
    "    if check_tokens(context) >= MAX_TOKENS:\n",
    "        context.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3926a7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_ask(\"1부터 100까지 소수를 찾는 파이썬 코드를 알려 줘.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f489e16-0836-4fcf-bfce-5046d7775a1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "single_ask(\"그래서 1부터 100까지 소수는 어떤 것들이 있어?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6265c471-2a8b-454d-8d6b-540bbd323c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_prime(n):\n",
    "    if n <= 1:\n",
    "        return False\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "prime_numbers = [num for num in range(1, 101) if is_prime(num)]\n",
    "print(prime_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9bf002c",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = [] # 리스트\n",
    "init_msg = '''\n",
    "아래의 기사를 토대로 코로나19 백신에 대한 부작용에 대해 나와 토론하려고 해.\n",
    "먼저 네가 \"안녕. 너의 의견은 뭐니?\"라고 시작해 줘.\n",
    "서로 한 번씩 의견을 돌아가면서 제시하게 될 거야.\n",
    "그리고 내가 자신의 의견을 말하면 너는 그것에 반대해서 계속해서 증거를 제시하면서 토론해.\n",
    "\n",
    "기사: 코로나19 백신이 심장 관련 염증이나 뇌혈전 등의 부작용과 연관이 있다는 연구 결과가 나왔다. \n",
    "19일(이하 현지시간) 블룸버그에 따르면 글로벌 백신 데이터 네트워크(GVDN)가 코로나19 백신 접종과 관련해 소수가 신경계, 혈액, 심장 관련 부작용과 연관이 있다고 전했다. \n",
    "이 연구는 8개국 9900만명의 백신 접종자를 대상으로 진행됐다. 연구진은 접종자들에게서 코로나19 백신 주요 부작용으로 간주되는 13가지 질환을 집중 조사해 코로나 백신접종과 \n",
    "부작용 간의 상관관계를 연구했다. 그 결과 심장 근육의 염증을 뜻하는 '심근염'은 mRNA(메신저 리보핵산) 백신을 1차~3차 접종한 환자들에게서 지속적으로 확인됐다. \n",
    "이 증상은 모더나 백신 주사를 두 번째 접종한 후 가장 많이 나타났다. '박제브리아'라 일컫는 아스트라제네카 백신은 길랑-바레 증후군과 연관이 있었다. \n",
    "이외 뇌혈전의 일종인 뇌정맥동 혈전증의 증가와도 연관이 있었다. 당초 연구진은 이 질환의 발병 건수를 66건으로 예상했지만 이를 상회하는 190건의 사례를 확인했다. \n",
    "블룸버그에 따르면 코로나19 백신은 지난 3년 동안 총 135억회 이상 투여됐다. 유럽에서만 100만명 이상의 고위험군 환자들이 코로나19 팬데믹 중에도 생명을 건질 수 있었다. \n",
    "그럼에도 백신 접종자 중 소수는 여전히 부작용을 호소하고 있는 만큼 코로나19 백신과 관련한 논쟁은 지속되고 있다.\n",
    "'''\n",
    "client = OpenAI(api_key = os.getenv('OPENAI_API_KEY'))\n",
    "\n",
    "def debate():\n",
    "    is_start = True\n",
    "    \n",
    "    while(1):\n",
    "        if not is_start:\n",
    "            message = input('Human: ')\n",
    "            message = message.strip()\n",
    "    \n",
    "            if message == '':\n",
    "                print('Input your text.')\n",
    "                continue\n",
    "            elif message == 'exit':\n",
    "                break\n",
    "    \n",
    "            # 대화 맥락을 고려하여 전체 최대 토큰을 초과하는지 체크하도록 한다.\n",
    "            total_cnt = check_tokens(context) + len(message)\n",
    "\n",
    "            if total_cnt >= MAX_TOKENS:\n",
    "                context.clear()\n",
    "                print('context cleared.')\n",
    "        else:\n",
    "            is_start = False\n",
    "\n",
    "        # ChatGPT 대화를 위한 메시지 형태 설정하기\n",
    "        if len(context) == 0:\n",
    "            context.append({\"role\": \"system\", \"content\": \"너는 훌륭한 토론 선생님이야.\"})\n",
    "            context.append({\"role\": \"user\", \"content\": init_msg})\n",
    "        else:\n",
    "            context.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        response = client.chat.completions.create(model=MODEL, messages=context, temperature=TEMPERATURE)\n",
    "        answer = response.choices[0].message.content\n",
    "        print(f\"AI: {answer}\")\n",
    "        \n",
    "        context.append({'role': 'assistant', 'content': answer})\n",
    "        \n",
    "        if check_tokens(context) >= MAX_TOKENS:\n",
    "            context.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dc33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "debate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1951c5",
   "metadata": {},
   "source": [
    "## How to use ChatGPT for textual data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c98db6-8a33-4102-a937-29b38d71f00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "보기는 학생이 작성한 글이야. \n",
    "툴민의 논증(Toulmin's Argumentation Pattern)에 따라 글에 포함되어 있는 주장(claim), 반박(rebuttal), 자료(data), 보장(warrant), 뒷받침(backing), 제한 조건(qualifer)이 명시적으로 포함되어 있는지 확인하고 추출해. \n",
    "만약 관련된 요소가 글 속에 포함되어 있지 않다면 '없음'으로 표시해. \n",
    "논증 요소의 정의는 다음과 같아.\n",
    "주장(claim): Assertions about what exists or values that people hold. \n",
    "자료(data): Statements that are used as evidence to support the claim. \n",
    "보장(warrant): Statements that explain the relationship of the data to the claim. \n",
    "제한 조건(qualifier): Special conditions under which the claim holds true. \n",
    "뒷받침(backing): Underlying assumptions that are often not made explicit. \n",
    "반박(rebuttal): Statements that contradict either the data, warrant, backing or qualifier of an argument. \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_data(text):\n",
    "    is_first = True\n",
    "    \n",
    "    try:\n",
    "        text = text.strip()\n",
    "        print('Original:', text)\n",
    "        query_msg = query + '\\nText:' + text\n",
    "\n",
    "        # 메시지 설정하기\n",
    "        messages = [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": query_msg}\n",
    "        ]\n",
    "\n",
    "        # ChatGPT API 호출하기\n",
    "        response = client.chat.completions.create(model=MODEL, messages=messages, temperature=TEMPERATURE)\n",
    "        answer = response.choices[0].message.content\n",
    "        answer = answer.strip()\n",
    "\n",
    "        print(answer)\n",
    "        \n",
    "        return answer\n",
    "        \n",
    "    except openai.APIError as e:\n",
    "        #Handle API error here, e.g. retry or log\n",
    "        print(f\"OpenAI API returned an API Error: {e}\")\n",
    "        return 'Error'\n",
    "        \n",
    "    except openai.APIConnectionError as e:\n",
    "        #Handle connection error here\n",
    "        print(f\"Failed to connect to OpenAI API: {e}\")\n",
    "        return 'Error'\n",
    "        \n",
    "    except openai.RateLimitError as e:\n",
    "        #Handle rate limit error (we recommend using exponential backoff)\n",
    "        print(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "        return 'Error'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51ce3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = '''\n",
    "I can compare the cell to a factory because cells also have their own functions and what or must to do just like in factory.\n",
    "If there's a one cell or one thing that is not functioning, it will not work.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec6315",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = inspect_data(sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e03383-8dfa-4b3a-bb70-94d575b2b0f1",
   "metadata": {},
   "source": [
    "### Using Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcca729-4b79-4a18-b5ce-61a53f86a78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    {\n",
    "        \"name\" : \"extract_element\",\n",
    "        \"description\": \"Extract the elements of argumentation from the text.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\", \n",
    "            \"properties\": {\n",
    "                \"Claim\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find claim from the text.\"\n",
    "                },\n",
    "                \"Data\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find data from the text.\"\n",
    "                },\n",
    "                \"Warrant\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find warrant from the text.\"\n",
    "                },\n",
    "                \"Backing\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find backing from the text.\"\n",
    "                },\n",
    "                \"Qualifier\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find qualifier from the text.\"\n",
    "                },\n",
    "                \"Rebuttal\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Find rebuttal from the text.\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83badabc-c5eb-4e49-a294-b469c6b3151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_data(text):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    sent = text\n",
    "    sent = sent.replace('\\n', ' ').strip()\n",
    "\n",
    "    query_msg = query + '\\nText:' + sent\n",
    "    #query_msg = query + txt_def + '\\nText:' + sent\n",
    "\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"너는 텍스트로부터 논증 요소를 분석하는 훌륭한 전문가야.\"},\n",
    "            {\"role\": \"user\", \"content\": query_msg}\n",
    "    ]\n",
    "        \n",
    "    response = client.chat.completions.create(model=MODEL,\n",
    "                                              messages=messages,\n",
    "                                              temperature=TEMPERATURE,\n",
    "                                              functions=functions, \n",
    "                                              function_call = {\"name\": functions[0][\"name\"]})\n",
    "    answer = response.choices[0].message.function_call.arguments\n",
    "    answer = json.loads(answer)\n",
    "\n",
    "    arguments = {}\n",
    "\n",
    "    arguments['Answer'] = sent\n",
    "    arguments['Claim'] = answer.get('Claim', '')\n",
    "    arguments['Data'] = answer.get('Data', '')\n",
    "    arguments['Warrant'] = answer.get('Warrant', '')\n",
    "    arguments['Backing'] = answer.get('Backing', '')\n",
    "    arguments['Qualifier'] = answer.get('Qualifier', '')\n",
    "    arguments['Rebuttal'] = answer.get('Rebuttal', '')\n",
    "    \n",
    "    print('Elapsed time:', \"{:.2f}\".format(time.time() - start_time))\n",
    "    \n",
    "    return arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d49e3-518b-4efb-9524-746c1144b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = inspect_data(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d7d65-b97a-470c-aa40-741dd30859cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1315b7-9c4c-4cdc-9ab4-693d8efbf179",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
